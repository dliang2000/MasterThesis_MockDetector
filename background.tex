%======================================================================
\chapter{Background}
\label{chap:background}
%====================================================================== 

\section{Dataflow Analysis}

Dataflow analysis is normally performed on top of a program's control flow graph (CFG). A CFG is a directed graph with each node stands for a statement in the program (the statement could be an assignment statement, an if statement, etc.), and the set of directed edges represents the overall control flow of the program.

Dataflow analysis, meanwhile, is a process of computing and gathering all the facts at each program point (i.e., each node in the CFG). The facts would usually be a mapping between program variables and the abstractions specifically defined to solve the problem at hand. 

For each dataflow problem, researchers or developers usually make two decisions before implementations. First, they would decide if the problem should be categorized as a forward or backward dataflow problem. For a forward dataflow analysis, the facts propagate along the direction of the control flow. On the other hand, the facts propagate in the opposite direction from the control flow in a backward dataflow analysis. (Few dataflow problems may require bidirectional flows.)

Researchers or developers also need to decide on if it is a may or must dataflow problem. The core difference between the two types of problems is how they handle the facts at all the join points in the program, where multiple branches meet. A may dataflow analysis keeps facts that hold true on any joined path, whereas a must dataflow analysis only keeps facts that hold true from all the branches.

Dataflow analysis is an imperative approach when solving a problem. It focuses on the ``HOW" part of the solution, providing a set of commands or operations to tackle the problem.

\subsection{Soot}

Among all the dataflow analysis tools, Soot~\cite{Vallee-Rai:1999:SJB:781995.782008} is a representative, IFDS-based Java optimization framework (where IFDS stands for Interprocedural, finite, distributive, subset problems), which provides a total of four types of intermediate representations to analyze Java bytecode. 

An intermediate representation (IR) is an abstract language designed for machines with no specifications. A good IR is independent of the source and target languages, and thus convenient to translate into code for the retargetable architecture.

For our project, we use Jimple (Java's simple), which is a stackless, typed 3-address IR in the CFG.

\section{Declarative Analysis}

While an imperative approach focuses on the "HOW" component of a solution, declarative analysis focuses on the "WHAT" part during the implementation. It gives out a set of orders for the framework to achieve.

It worths noting that a declarative approach normally comes with an underlying imperative layer, where some type of tool handles the imperative processes for you.

\subsection{Doop}

We choose Doop among many declarative frameworks for our project. It comes with a batch of base "analyses expressed in the form of Datalog rules"~\cite{doop-repo}. The current version uses Soufflé\footnote{\url{https://souffle-lang.github.io/docs.html}}

Since Doop is a declarative framework, its underlying imperative layer is the auto-generated input facts from Soot framework, which consequently imported to Doop's database. These input facts are then processed by the implemented datalog rules (on top of the selected Soufflé base analysis).


\section{Mock Objects} 

Mock objects are commonly used in unit test cases. They substitute for the real objects that are normally hard to create (e.g., a database), or slow to process (e.g., a connection). It is noteworthy that mock objects are never being tested in test cases, their purpose are to stand in for the dependencies, assisting for the behavioural test of the real object. For this to happen, the mock objects must at least mimic the behaviour at interfaces connecting to the real object under test.

For our project, we consider for Java mock source methods from three mocking frameworks: EasyMock\footnote{\url{https://easymock.org/}}, Mockito\footnote{\url{https://site.mockito.org/}}, and PowerMock\footnote{\url{https://github.com/powermock/powermock}}. According to a prior study~\cite{mostafa14:_empirical_study_mock_frameworks}, EasyMock and Mockito are used in about 90\% of the 5,000 randomly sampled projects. We then added a third mocking framework by our own choice. Thus, we believe our analysis results should be applicable to most of the Java benchmarks. 

\subsection{Mock Objects in IR} 

% Do I move Figure 3.1, Listing 3.2, and the discussions here? We need some motivation in Background if we are going to remove chapter Motivating Examples

\section{Preliminary Research}

\begin{figure}[h]
	\centering
	\input{withZone.tikz}
	\caption[Caption for SiblingClass untested method.]{joda-time contains superclass \textit{AssembledChronology} and subclasses \textit{BuddhistChronology}, \textit{GJChronology}, \textit{StrictChronology}, and others. Method \textit{withZone()} is tested in \textit{Buddhist-} and \textit{GJChronology} but not \textit{StrictChronology}.\footnotemark}
	\label{fig:hierarchyView}
\end{figure}

\footnotetext{The content of this figure have been incorporated within a NIER paper published by IEEE in 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME), available online: https://ieeexplore.ieee.org/xpl/conhome/9240597/proceeding[doi: 10.1109/ICSME46990.2020.00075]. Qian Liang and Patrick Lam, "SiblingClassTestDetector: Finding Untested Sibling Functions"}

Prior to the development for \textsc{MockDetector}, we were working on a project to detect untested functions that have analogous implementations in sibling classes (they share a common superclass), where at least one of the related implementations are tested. The overall goal of that project is to reduce untested code. Though testing could not guarantee desired program behaviour, developers certainly know nothing about any untested code. Since the sibling methods share the same specification, it is likely that a unit test case covering for one sibling class's implementation may also work for the untested after small modifications, potentially increasing the statement coverage and consequently having a better chance to gain behavioural insight of the benchmark.

Figure~\ref{fig:hierarchyView} illustrates such an example from an open-source benchmark, joda-time (version 2.10.5). The abstract class \textit{AssembledChronology} inherits the specification of method \textit{withZone()} from its parent class, which is not shown in the Figure. \textit{AssembledChronology}'s subclasses \textit{BuddhistChronology}, \textit{GJChronology}, \textit{StrictChronology}, and many others are at the same hierarchy level, which are defined as sibling classes. These sibling classes all have an implementation of \textit{withZone()}; however, the \textit{withZone()} implementations in \textit{BuddhistChronology} and \textit{GJChronology} are tested, whereas the implementation in \textit{StrictChronology} is not.

As the research progressed, we encountered the test case in Listing~\ref{lis:siblingMethodCall}, and realized that it would be a necessary step to remove method invocations on mock objects from the call graph generated by existing static analysis frameworks, as otherwise we may mistakenly treat such test case as the one covering for the tested sibling method, since the existing static analyses tools could not distinguish method invocations on mock objects from method invocations on real objects. 

\begin{lstlisting}[basicstyle=\ttfamily, caption={This code snippet illustrates an example from commons-collections4, where the method \textit{addAll()} invoked on the mock object \texttt{c} could be mistreated as a focal method being covered by existing static analysis frameworks.},
basicstyle=\ttfamily,language = Java, framesep=4.5mm, escapechar=|,
framexleftmargin=1.0mm, captionpos=b, label=lis:siblingMethodCall, morekeywords={@Test}]

@Test
public void addAllForIterable() {
final Collection<Integer> inputCollection = createMock(Collection.class);
...
final Collection<Number> c = createMock(Collection.class);
...
expect(c.addAll(inputCollection)).andReturn(false);
}
\end{lstlisting}