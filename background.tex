%======================================================================
\chapter{Background}
\label{chap:background}
%====================================================================== 

Static analysis techniques are an important tool for code analysis, especially when analyzing benchmarks with large code bases. Running an applicaion does not always expose bugs (for instance, due to missing branch coverage), where static analysis tools are more competent. A good static analysis tool can help developers to find bugs that are hard to spot manually (e.g., array out of bounds exception), which reduces time and costs involved in fixing bugs. 

\section{Dataflow Analysis}

Dataflow analysis is normally performed on top of a program's control flow graph (CFG). A CFG is a directed graph with each node stands for a statement in the program (the statement could be an assignment statement, an if statement, etc.), and the set of directed edges represents the overall control flow of the program.

Dataflow analysis, meanwhile, is a fixed point algorithm that computes and gathers all the facts at each program point (i.e., each node in the CFG). The facts would usually be a mapping between program variables and the abstractions specifically defined to solve the problem at hand. 

For each dataflow problem, researchers or developers usually make two decisions before implementations. First, they would decide if the problem should be categorized as a forward or backward dataflow problem. For a forward dataflow analysis, the facts propagate along the direction of the control flow. Determining whether expressions are available at each program point is a type of forward dataflow problem. On the other hand, the facts propagate in the opposite direction from the control flow in a backward dataflow analysis, and most importantly, we need to access the future usage of the variables. Analyzing the liveness of variables is a known backward dataflow problem. (Few dataflow problems may require bidirectional flows.)

Researchers or developers also need to decide on if it is a may or must dataflow problem. The core difference between the two types of problems is how they handle the facts at all the join points in the program, where multiple branches meet. A may dataflow analysis keeps facts that hold true on any joined path. "Reaching Definitions" is a may analysis problem. It checks if a definition (or assignment) of a variable reaches a specific program point. On the other hand, must dataflow analysis only keeps facts that hold true from all the branches. Determining available expressions is also a must analysis problem.

Dataflow analysis is an imperative approach when solving a problem. It focuses on the ``HOW" part of the solution, providing a set of commands or operations to tackle the problem.

\subsection{Soot}

Among all the dataflow analysis tools, Soot~\cite{Vallee-Rai:1999:SJB:781995.782008} is a representative, IFDS-based Java optimization framework (where IFDS stands for Interprocedural, finite, distributive, subset problems), which provides a total of four types of intermediate representations to analyze Java bytecode. 

An intermediate representation (IR) is an abstract language designed for machines with no specifications. A good IR is independent of the source and target languages, and thus convenient to translate into code for the retargetable architecture.

For our project, we use Jimple (Java's simple), which is a stackless, typed 3-address IR in the CFG.

\section{Declarative Analysis}

While an imperative approach focuses on the ``HOW" component of a solution, declarative analysis focuses on the ``WHAT" part during the implementation. It gives out a set of orders for the framework to achieve.

It worths noting that a declarative approach normally comes with an underlying imperative layer, where some type of tool handles the imperative processes for the developer.

\subsection{Doop}

We choose Doop among many declarative frameworks for our project. It comes with a batch of base ``analyses expressed in the form of Datalog rules"~\cite{doop-repo}. The current version uses Soufflé\footnote{\url{https://souffle-lang.github.io/docs.html}} as the datalog engines for analyses rules.

Since Doop is a declarative framework, its underlying imperative layer is the auto-generated input facts from Soot framework, which consequently imported to Doop's database. These input facts are then processed by the implemented datalog rules (on top of the selected Soufflé base analysis). Doop also computes fixed points like any dataflow analysis tools.


\section{Mock Objects} 

Mock objects are commonly used in unit test cases. They substitute for the real objects that are normally hard to create (e.g., a database), or slow to process (e.g., a connection). It is noteworthy that mock objects are never being tested in test cases. Their purpose are to stand in for the dependencies, assisting for the behavioural test of the real object. For this to happen, the mock objects must at least mimic the behaviour at interfaces connecting to the real object under test.

For our project, we consider for Java mock source methods from three mocking frameworks: EasyMock\footnote{\url{https://easymock.org/}}, Mockito\footnote{\url{https://site.mockito.org/}}, and PowerMock\footnote{\url{https://github.com/powermock/powermock}}. According to a prior study~\cite{mostafa14:_empirical_study_mock_frameworks}, EasyMock and Mockito are used in about 90\% of the 5,000 randomly sampled projects. We then added a third mocking framework by our own choice. Thus, we believe our analysis results should be applicable to most of the Java benchmarks. 