%======================================================================
\chapter{Discussion}
\label{chap:discussion}
%====================================================================== 

Having described our imperative and declarative approaches to implementing mock analysis, we now comment on the strengths and weaknesses of these two approaches. We hope that our discussion will help future designers of source code analyses and frameworks.

\section{Subsequent Use of Results} 

Doop is a standalone tool. It depends on other tools to provide input, but provides output in the form of \texttt{csv} files, whose content can be matched to the program source, if a subsequent analysis has the appropriate internal representation. On the other hand, Soot is a compiler framework. Thus, using the Soot analysis results in a subsequent compilation phase is quite easy. Doop works quite well for producing analysis results, and not quite as well for using these results in a compilation process. Our Soot analysis also doesn't need to process the whole program for itself to produce the analysis results that we're interested in here---our intraprocedural analysis can use the existing in-memory representation and pass it on to the next phase, while Doop reads the whole program, throws it away, and leaves nothing for the next compilation phase. 

\section{Expressiveness vs Concision}

In~\cite{bravenboer09:_stric_declar_specif_sophis_point_analy}, Bravenboer and Smaragdakis point out that:
\begin{quote}
	Even conceptually clean program analysis algorithms that
	rely on mutually recursive definitions often get transformed
	into complex imperative code for implementation purposes.
\end{quote}
The presentation of the declarative approach in Chapter~\ref{chap:technique} could meaningfully include direct excerpts from the Datalog; including Java code is rarely meaningful, as there is too much boilerplate in that language.

The declarative approach takes 237 non-comment lines, compared to about 533 non-comment lines for the main part of the imperative approach, which is a significant point in favour of Doop. A head-to-head comparison is tricky, as the imperative approach also uses pre-analyses which are not present in the declarative approach.

We comment on the reasons for using helper analyses in the imperative version and not the declarative version. Recall that the helper analyses pre-computed information about 1) mock annotations and 2) constructors and setup methods. The mock annotations are an inessential difference; they could be computed on the fly in the imperative version, as they are in the declarative version. As for the constructors: when thinking imperatively, it is more intuitive to explicitly order the computations for constructors before regular test methods. On the other hand, thinking declaratively, it is more natural to use mutual recursion to declare a dependency on the results of previous computations for fields (our relation \texttt{isCollectionFieldThatContainsMocks} in particular) than to declare an explicit ordering. There is a small semantic difference in the two implementations, as the declarative implementation does not require field writes to be confined to constructors and setup methods; in this particular case, we empirically verified that the imperative assumption was almost always satisfied.

We also contrast how we store the abstraction in the two versions. The imperative version uses a standard dataflow analysis abstraction (three bits per local variable/field reference), along with an explicitly specified merge operator, while the declarative version uses one relation for each of the three bits. Propagating and merging data happens automatically in Doop.
% There is something going on with doop and kills, but I don't know enough of what's happening to meaningfully comment on it. Intersection-based analyses seem to be possible, because there is e.g. IntraproceduralMustPointTo. There's also something to do with phi nodes and redefinitions, but I can't clearly express it.

Another difference between the declarative and imperative versions is in the support for interprocedural analysis. As stated earlier, in Chapter~\ref{chap:technique}, the declarative version implements a context-insensitive interprocedural analysis while the imperative version is intraprocedural. The choice of intraprocedural versus interprocedural depends strongly on the particular analysis being implemented. Implementing the interprocedural analysis declaratively was impressively easy, while it is significantly more challenging to implement an interprocedural analysis in Soot, requiring the use of Heros~\cite{soap12ifds}, an additional framework. On the other hand, the Heros implementation would be IFDS-based and be context-sensitive; it would be somewhat harder to upgrade our context-insensitive implementation to a context-sensitive Doop implementation.

It is easier to add instrumentation, e.g. timers, to the imperative version than the declarative version. Doop contains some built-in timers, but it is unclear how to add new ones.

\section{Development Velocity}

To help the reader calibrate our descriptions, we describe our experience levels with Soot and Doop. I initially had little Soot experience, developed the Soot implementation through countless hours of testing and debugging, and took guidance and suggestions from my supervisor, who is an early code contributor to the Soot framework. My supervisor, the co-author of the submitted conference research paper, developed the Doop implementation.

Soot is a mature program analysis framework and many of the common sticking points have, over the years, been addressed by the developers. Nevertheless, it can be intimidating to start working with Soot. Our experience with Doop is that it is overall robust, yet still being actively developed (i.e. occasionally, at the start, some daily snapshots didn't work with some versions of the underlying Souffl√© engine). There is more documentation for Soot than for Doop, although even for Doop, it is often possible to scrape together answers to one's questions from the source code and the online documentation. Finding the right API (or relation, in Doop) to use can be challenging for both Soot and Doop; it's impossible for us to fairly compare them, due to our different experiences with Soot and Doop.

We thank the Doop developers for their timely and helpful answers to our questions; developer or community support is necessary to successfully use Doop (or, for that matter, most research-grade program analysis frameworks, including Soot).

Most of the time, adding a feature to the declarative version (e.g. field support) required an evening of work. This typically happened first; the declarative version is better for cleanly describing some approximation of the desired behaviour. Somewhat to our surprise, it was then possible to fast-follow with the imperative version, which ended up not taking much more than an evening to implement either. We believe that the existence of the declarative specification helped with designing the imperative version.

The declarative version was still subject to the combinatorial feature interaction problem; for instance, when we added support for fields and containers, we also specifically needed to add support for containers stored in fields.

Debugging is an inevitable part of any development process, including this one; declarative languages are no proof against debugging. Some Doop errors were just frustrating, e.g. hardcoding a syntactically incorrect method signature for a collection method. Other times, better type system enforcement in Datalog, and in particular, identifying relations that are unsatisfiable due to type conflicts, would help. Soot errors are typical programming errors.

Iteration speed can help with more effective debugging. On some benchmarks, Soot iterations could finish in under a minute, while Doop analysis-only iterations could finish in 10 seconds (but we didn't know that at the time). To expand on that: while developing our analysis, we ran our analysis together with the main analysis, and recomputed the main analysis every time we iterated. Yet, Doop supports running add-on analyses like ours, in isolation, after the main analysis terminates. If we were doing it again, we would develop our analysis as a run-after analysis. Running with the main analysis requires at least a 2.5-minute iteration time due to the necessity of re-compiling and re-running the entire analysis every time the analysis changes, while running an analysis after the main analysis can take 10 seconds, as mentioned above. Setting up the analysis to run after the main analysis is trickier and requires understanding of Doop which we did not have until late in the process. 

As stated above, instrumentation is easier in Soot than in Doop, and that extends to printing debug information and using traditional debugging tools, which works as well for Soot as traditional debugging does in general. To debug the Doop analysis, we resorted to outputting relevant relations after a Doop run and manually pinpointing which facts were missing or extraneous. Because Doop uses Soot to generate program facts, understanding Soot in particular and compilers in general was invaluable while developing the Doop implementation---we also looked at the Soot intermediate representation to understand what analysis information was flowing to which intermediate variables.


\section{Future Work} 
\label{sec:future-work}

In this project, we presented two implementations for detecting mock objects and tracking mock invocations in test suites. There are multiple areas require further investigation to improve \textsc{MockDetector}'s quality:

\begin{itemize}
	\item We will work on Soot interprocedural implementations in our tool for a more complete mock analysis. It will be useful to compare and further understand the two approaches analyzing mock objects.
	\item The current interprocedural Doop implementation could not differentiate method invocations in the test suites from the ones in the source code. We will work on modifying Doop's implementation to only report results from the test suites for move accurate results.
	\item We plan to build construct an automated focal method analysis tool. It aims to help quantitatively evaluate our \textsc{MockDetector} tool in assisting for the focal method findings.
\end{itemize}


\section{Summary} 

We would conclude that, especially with the knowledge we have now gained about Doop, prototyping in Doop is easier than in Soot, but that it is no panacea; it remains subject to the feature interaction problem as well as debugging. Additionally, trying to add certain functional behaviours to the Doop implementation, such as timers, can be challenging.

Overall, in this thesis, we have described a case study of the \textsc{MockDetector} static analysis, which we intend to use for further static analyses of test cases. We have implemented \textsc{MockDetector} twice---once imperatively in Soot, and once declaratively in Doop---and characterized its performance on a test suite. Finally, we have discussed our experience implementing this analysis twice, and pointed out the benefits and disadvantages of the imperative and declarative approaches for writing static analyses.