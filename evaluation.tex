%======================================================================
\chapter{Evaluation}
\label{chap:evaluation}	
%====================================================================== 

%The major difference between the Doop's total runtime and the actual time spent on mock invocation analysis comes from the build of the complete graph. %Add reference for SLOCCount.
In this chapter, we perform a qualitative evaluation of our \textsc{MockDetector} tool and its potential contribution to finding focal methods, as well as a quantitative evaluation of the tool's effectiveness in identifying mock objects and finding invocations on them. We compare running times and efficacy between our Soot and Doop implementations. In addition, we also investigate four Doop base analyses and the effect of the base analysis on running times and efficacy.

\section{Qualitative Evaluation}

We plan to reproduce Ghafari's algorithm~\cite{ghafari15:_autom} to automatically identify focal methods, and see how much it would benefit from mock removal. However, first, before implementing it, we use manual inspection to evaluate how necessary it is to remove mocks from consideration as focal methods. We present two examples in this section to showcase how our tool eliminates method invocations that are definitely not focal methods.

We begin by revisiting the example first discussed in Section~\ref{sec:running-example}. Figure~\ref{fig:mockExampleEvaluation} shows the process of locating the mock object \texttt{session} and thus the mock invocation \texttt{getRequest()} in the example. Both the Soot and Doop implementations report one mock invocation from the test case. Soot outputs the Invoke Expression for \texttt{getRequest()}, whereas Doop outputs the corresponding call-graph edge in the isMockInvocation relation. With the assistance of \textsc{MockDetector}, we could remove \texttt{getRequest()} from consideration as a focal method. We judge \texttt{getToolchainsForType()} to be the focal method because it is the only method invocation remaining after the elimination process. In addition, as the \texttt{length} attribute of the return object \texttt{basics} from the invocation \texttt{getToolchainsForType()} is checked in the assertion statement on Line 13, this test case indeed tests the behaviour of \texttt{toolchainManager.getToolchainsForType()}.

For this example, since Ghafari's algorithm does not consider accessors (or so-called inspector methods in the paper) as focal methods, they will presumably report no focal method for this unit test case. This clearly is an incorrect result from Ghafari's algorithm.

Figure~\ref{fig:mockExample2Evaluation} displays the second example, a test from \textsc{vraptor-core} with multiple mock invocations. The field mock objects are defined via mock annotations. 

Both the Soot and Doop implementations identify all the field mocks, and consequently report three mock invocations from the test case. They also successfully output the Invoke Expressions or the corresponding call-graph edges for the three mock invocations: \texttt{getContentType()}, \texttt{deserializerFor()}, and \texttt{unsupportedMediaType()}. By a process of elimination, we could deduce that \texttt{intercept()} on Line~\ref{line:focal-method} is the focal method. 

The heuristic of Ghafari's algorithm requires the unit test case to have at least one assertion statement. Since this unit test case does not have any assertion statement, Ghafari's algorithm will presumably have undefined behaviour analyzing this unit test.

In summary, Ghafari's algorithm does not seem to have a provision for methods with no mutators (presumably returns empty set), nor does it seem to be able to handle test cases with no assertions (presumably undefined behaviour).

Both examples qualitatively demonstrate that our \textsc{MockDetector} tool can help remove invocations on mocks from consideration as focal methods, thus making it easier to identify focal methods. We believe our tool could augment the precision of Ghafari's algorithm. We could also potentially construct a novel elimination-based algorithm. Such an algorithm might be better since it would consider methods that are actually focal methods, like sophisticated getters.

\begin{figure}[h]
	\begin{lstlisting}[
	basicstyle=\ttfamily,language = Java, framesep=4.5mm, framexleftmargin=1.0mm, captionpos=b, escapechar={|}, mathescape=true, morekeywords={@Test}]
	@Test
	public void testMisconfiguredToolchain() throws Exception {
		//        		mock:|\cmark|	 mockAPI:|\cmark|
		MavenSession |\colorbox{olive}{session}| = |\colorbox{teal}{mock}| ( MavenSession.class );
		MavenExecutionRequest req = new DefaultMavenExecutionRequest();
		//     mock invocation:|\cmark| $\Rightarrow$ focal method:|\xmark|
		when( session.|\colorbox{red}{getRequest()}| ).thenReturn( req ); |\label{line:mock}|
		
		ToolchainPrivate[] basics =
			//     					 	focal method:|\cmark|
			toolchainManager.|\colorbox{gray}{getToolchainsForType}|("basic", session); |\label{line:real}|
		
		assertEquals( 0, basics.length );
	}
	\end{lstlisting}
	
	\caption{Qualitative evaluation of removing one mock invocation from focal method consideration.}
	\label{fig:mockExampleEvaluation}
\end{figure}


\begin{figure}[p]
	\begin{lstlisting}[
	basicstyle=\fontsize{10}{12}\ttfamily, language = Java, framesep=4.5mm, framexleftmargin=1.0mm, captionpos=b, escapechar={|}, mathescape=true, morekeywords={@Test}]
	// Fields used in the unit test case
	private DeserializingInterceptor interceptor;
	// mock annotation:|\cmark|
	|\colorbox{teal}{@Mock}| private HttpServletRequest request;
	// mock annotation:|\cmark|
	|\colorbox{teal}{@Mock}| private InterceptorStack stack;
	// mock annotation:|\cmark|
	|\colorbox{teal}{@Mock}| Deserializers deserializers;
	private MethodInfo methodInfo;
	// mock annotation:|\cmark|
	|\colorbox{teal}{@Mock}| Container container;
	// mock annotation:|\cmark|
	|\colorbox{teal}{@Mock}| private Status status;
	
	@Before
	public void setUp() throws Exception {
	MockitoAnnotations.initMocks(this);
	
	methodInfo = new DefaultMethodInfo();
	interceptor = new DeserializingInterceptor(request, deserializers, methodInfo, container, status);
	}
	
	
	@Test
	public void willSetHttpStatusCode415IfThereIsNoDeserializerButIsAccepted() throws Exception {
		//     mock invocation:|\cmark| $\Rightarrow$ focal method:|\xmark|
		when( request.|\colorbox{red}{getContentType()}| ).thenReturn("application/xml");
		//     		mock invocation:|\cmark| $\Rightarrow$ focal method:|\xmark|
		when( deserializers.|\colorbox{red}{deserializerFor}|("application/xml", container) ).thenReturn(null);
		
		//       focal method:|\cmark|
		interceptor.|\colorbox{gray}{intercept}|(stack, consumeXml, null); |\label{line:focal-method}|
		//     		mock invocation:|\cmark| $\Rightarrow$ focal method:|\xmark|
		verify(status).|\colorbox{red}{unsupportedMediaType}|("Unable to handle media type [application/xml]: no deserializer found.");
		verifyZeroInteractions(stack);
	}
	\end{lstlisting}
	
	\caption{Qualitative evaluation of removing multiple mock invocations from focal method consideration.}
	\label{fig:mockExample2Evaluation}
\end{figure}

\section{Description of Benchmark Suite}

We have quantitatively evaluated \textsc{MockDetector} on 8 open-source benchmarks, along with a micro-benchmark that we developed to test our tool. We ran all of our experiments on a 32-core Intel(R) Xeon(R) CPU E5-4620 v2 at 2.60GHz with 128GB of RAM running Ubuntu 16.04.7 LTS.

Table~\ref{tab:runtimes} presents summary information about our benchmarks and run-times, namely the LOC and Soot and Doop analysis run-times for each benchmark. The 9 benchmarks include over 383 kLOC, with 184 kLOC in the test suites, as measured by SLOCCount\footnote{\url{https://dwheeler.com/sloccount/}}. The Soot total time is the amount of time that it takes for Soot to analyze the benchmark and test suite in whole-program mode, including our analyses. The Soot intraprocedural analysis time is the sum of run-times for the main analysis plus two pre-analyses, as described in Section~\ref{sec:soot}. Meanwhile, the reported Doop run-time is from the context-insensitive analysis, while the Doop analysis time for intraprocedural mock invocation analysis is for running the analysis alone based on recorded facts from the benchmark. The total Doop run-time is much slower than the total Soot run-time because Doop always computes a call-graph, which is an expensive operation. We believe that the Doop analysis-only time is also slower because it computes a solution over the entire program, as opposed to Soot, which works one method at a time.

\section{Field Mocks}

We perform an evaluation on the necessity of our pre-analyses finding field mocks. 
Table~\ref{tab:field-mocks} displays the number of field mock objects that are defined via \texttt{@Mock} annotations, in the constructors, and in the \texttt{@Before}/\texttt{setUp()} methods, respectively. 

We focus on the 5 open-source benchmarks that have defined field mock objects. They are \textsc{bootique}, \textsc{maven-core}, \textsc{jsonschema2pojo-core}, \textsc{mybatis}, and \textsc{vraptor-core}. Among these 5 benchmarks, \textsc{jsonschema2pojo-core}, \textsc{mybatis}, and \textsc{vraptor-core} have a high number (565) or a high percentage (over 50\%) of of test-related methods containing mock objects, and have many intraprocedural mock invokes. From the results collected in Table~\ref{tab:field-mocks}, we can tell these benchmarks also prefer to define field mock objects. Instead of repetitively creating the same mock objects in each test case within the same test class, these benchmarks create the field mock objects once and and consequently use them in all the test cases. This pattern reduces the need for code maintenance. In addition, although \textsc{bootique} and \textsc{maven-core} have lower number of tests using mock objects, these 2 benchmarks also prefer to define field mock objects. Therefore, 5 out of the 8 open-source benchmarks prefer to define field mock objects for the ease of testing. This suggests that our pre-analysis for field mocks described in Section~\ref{subsec:pre-analysis} is indeed a necessary and an effective step for analyzing mock objects and mock invocations in the test suites.

\section{Prevalence of Mocks}

We next investigated the prevalence of mocks. Table~\ref{tab:mocks} presents the number of test-related (Test/Before/After) methods which contain local variables or which access fields that are mocks, mock-containing arrays, or mock-containing collections, as reported by our Soot-based intraprocedural analysis. Across the 8 benchmarks, test-related methods containing local/field mocks or mock-containing containers accounted for 0.35\% to 51.8\% of the total number of test-related methods found in public concrete test classes. Our benchmarks are from different domains and created by different groups of developers. The difference in mock usage reflects their different philosophies and constraints regarding the creation and usage of mock objects in tests. Benchmarks like \textsc{vraptor-core} and \textsc{jsonschema2pojo-core} have more than half of their test-related methods containing mock objects (and mock-containing arrays); in both of these, most field mocks are created via annotations and reused in multiple test cases in the same class.

The core result of this thesis is in Table~\ref{tab:invokes}, which presents the number of method invocations on mocks detected by our implementations. We present numbers from the imperative intraprocedural Soot implementation, as well as a total of eight versions of the declarative Doop implementation\footnote{bootique, mybatis and vraptor timed out for Doop's 1-object-sensitive analysis without our mock analysis, and we report ``--'' for their times.}: \{ ``basic-only'' (class hierarchy analysis), ``context-insensitive'' (CI), ``context-insensitive-plusplus'' (CIPP), ``1-object-sensitive'' \} Doop base analysis $\times$ \{ intraprocedural, interprocedural \}. 

Note that our declarative and imperative implementations find exactly the same number of intraprocedural mock invocations for 4 of our benchmarks. On the others, the main source of missed mock calls in the Soot implementation is missing support for array- or collection-related functions. Our intraprocedural analysis finds that method invocations on mock objects account for a range from 0.086\% to 16.4\% of the total number of invocations. The two implementations of mock analysis serve to cross validate each other. Often there is only one implementation for a static analysis project, thus it is difficult to judge on the implementation's soundness and correctness. It is possible to formally prove analysis properties, but even then, nothing guarantees conformance of the implementation to the formal description. For this project, I can cross check the results from two implementations, investigate the discrepancies and decide which implementation to further improve on. The improvements on finding intraprocedural mock invocations will be in future work.

Combining the mock counts result from Table~\ref{tab:mocks} and intraprocedural mock invocations result from Table~\ref{tab:invokes}, we can see that benchmarks such as \textsc{jsonschema2pojo-core}, \textsc{mybatis} and \textsc{vraptor-core} rely quite heavily on mock objects in their tests, which supports our motivation that it is quite necessary to track mock objects and invocations on mocks, to refine existing algorithms to find focal methods and remove candidates that are definitely not focal methods. 

%--- discuss the numbers for the interprocedural analysis.

\subsection{Intraprocedural vs. Interprocedural}

In Section~\ref{sec:common} we discussed the implementation of our intraprocedural and interprocedural analyses. We can now discuss the effects of these implementation choices on the experimental results. Recall that we chose, unsoundly, to not propagate any information across method calls in the intraprocedural analysis. Thus, the intraproc columns in Table~\ref{tab:invokes} show smaller numbers than the interproc columns, as expected. Note also that there is a sometimes drastic increase from the intraprocedural to the interprocedural result, e.g. from 40 to 1300 for \textsc{flink}. This is because mocks can (especially context-insensitively) propagate from tests to the methods that they call and throughout the main program code. It would be desirable to be able to differentiate test helper methods, which we do want to propagate mocks to, from methods in the main program, which we generally do not want to propagate mocks to. (Although the main program may make mock invocations on objects it is given, we do not want to report these mock invocations from main code.) However, our current analysis infrastructure treats test and main code identically.

\section{Doop Analysis Results}

We explored the run-time performance of our 8 declarative analysis variants based on recorded program facts. We used hyperfine\footnote{\url{https://github.com/sharkdp/hyperfine}} to perform 10 benchmarking runs for the command that performs mock analysis, and present the means and standard deviations from the 10 runs in Table~\ref{tab:doop-intra-runtimes} and~\ref{tab:doop-inter-runtimes}.

The running times show that the Doop runs with basic-only base analysis spend more time on mock analysis for most benchmarks than the run-times from the more advanced base analyses. Basic-only is a naive base analysis that generates a much larger call graph than more advanced base analyses, and as it would need to check through more call graph edges, we expect that basic-only would generally spend more time on performing mock analysis. Table~\ref{tab:doop-callgraph-all-counts} highlights the number of source classes and target classes presented in the call graph generated by the four base analyses, which supports the idea that basic-only generates a bigger call graph, and spends most of the extra time checking over unnecessary classes generated in the call graph instead of performing actual mock analysis.

To further investigate the interprocedural running times on benchmarks including \textsc{jsonschema2pojo-core}, \textsc{mybatis}, and \textsc{vraptor-core}, we remove call graph edges that have library classes or dependency classes as source classes for each base analysis, and then count the total number of source classes and target classes in the filtered call graph. As Table~\ref{tab:doop-callgraph-package-counts} shows, if we take the difference of the number of target classes from the number of source classes for each base analysis, the results suggest that the more advanced base analyses (CI, CIPP) reach more target classes from a better defined subset of source classes that are application classes. 

\subsection{Investigating Basic-Only CHA}
\label{subsec:basic-only-cha}

We believe that the basic-only base analysis has some undesired behaviour in the process of generating the CHA. We have discussed with the Doop authors. They consider that calls to concrete implementations of abstract methods are not included in CHA. We believe this is not correct from our data collection but it is their assumption. Figure~\ref{fig:edgesToSiblingMethod} shows the call graph edges from the method \textit{isApplicableType} on the class PatternRule to \texttt{java.lang.String fullName()}, which is a sibling method (i.e. a method that has implementations in multiple classes under the same superclass) that is first declared in the abstract base class \texttt{com.sun.codemodel.JType}. From Figure~\ref{fig:edgesToSiblingMethod}, we can tell that the basic-only base analysis only reports edges to the (abstract) method \texttt{com.sun.codemodel.JType.fullName}, whereas the call graph generated by CI base analysis reports edges to implementation sites of the target method. We believe that this shows that there are missing edges in basic-only from its choice to stop at the abstract level. Combining this information with the total number of interprocedural mock invocations reported for the three benchmarks, we know that CI and CIPP have found notably more mock invocations in the process of searching through the edges to the implementation sites of the sibling methods, and the time spent on this process possibly accounts for the higher mock-analysis running times of CI and CIPP on benchmarks \textsc{jsonschema2pojo-core}, \textsc{mybatis}, and \textsc{vraptor-core}.


\begin{figure}
	\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, numbers=none, framesep=4.5mm, framexleftmargin=1.0mm, captionpos=b, escapechar=|]
	
	Call-graph Edge in Basic-only:
	0	<org.jsonschema2pojo.rules.PatternRule: boolean 
	isApplicableType(com.sun.codemodel.JFieldVar)>/com.sun.codemodel.JClass.fullName/0	
	0	<com.sun.codemodel.JType: java.lang.String fullName()>
	
	Call-graph Edges in CI:
	<<immutable-context>>	<org.jsonschema2pojo.rules.PatternRule: 
	boolean isApplicableType(com.sun.codemodel.JFieldVar)>/com.sun.codemodel.JClass.fullName/0
	<<immutable-context>>	<com.sun.codemodel.JTypeVar: java.lang.String fullName()>
	
	<<immutable-context>>	<org.jsonschema2pojo.rules.PatternRule: 
	boolean isApplicableType(com.sun.codemodel.JFieldVar)>/com.sun.codemodel.JClass.fullName/0	
	<<immutable-context>>	<com.sun.codemodel.JTypeWildcard: java.lang.String fullName()>
	
	<<immutable-context>>	<org.jsonschema2pojo.rules.PatternRule: 
	boolean isApplicableType(com.sun.codemodel.JFieldVar)>/com.sun.codemodel.JClass.fullName/0	
	<<immutable-context>>	<com.sun.codemodel.JNarrowedClass: java.lang.String fullName()>
	
	<<immutable-context>>	<org.jsonschema2pojo.rules.PatternRule: 
	boolean isApplicableType(com.sun.codemodel.JFieldVar)>/com.sun.codemodel.JClass.fullName/0	
	<<immutable-context>>	<com.sun.codemodel.JArrayClass: java.lang.String fullName()>
	
	<<immutable-context>>	<org.jsonschema2pojo.rules.PatternRule: 
	boolean isApplicableType(com.sun.codemodel.JFieldVar)>/com.sun.codemodel.JClass.fullName/0	
	<<immutable-context>>	<com.sun.codemodel.JDefinedClass: java.lang.String fullName()>
	
	<<immutable-context>>	<org.jsonschema2pojo.rules.PatternRule: 
	boolean isApplicableType(com.sun.codemodel.JFieldVar)>/com.sun.codemodel.JClass.fullName/0	
	<<immutable-context>>	<com.sun.codemodel.JDirectClass: java.lang.String fullName()>
	
	<<immutable-context>>	<org.jsonschema2pojo.rules.PatternRule: 
	boolean isApplicableType(com.sun.codemodel.JFieldVar)>/com.sun.codemodel.JClass.fullName/0	
	<<immutable-context>>	<com.sun.codemodel.JCodeModel\$JReferencedClass: java.lang.String fullName()>
	
	\end{lstlisting}
	\caption{The call graph edges to the method \textit{fullName()} from both basic-only and context-insensitive base analyses. The method is declared in abstract class \texttt{com.sun.codemodel.JType} and implemented in its children classes.}
	\label{fig:edgesToSiblingMethod}
	
\end{figure}
% More discussion will be added for the outlier: mybatis and vraptor interprocedural runtimes.

\subsection{Mock Invocations Results by Different Base Analyses}

The four base analyses report the same number of intraprocedural mock invocations in 8 benchmarks. The minor difference in \textsc{vraptor-core} is due to one method (which ought to be present) not showing up in Doop's context-insensitive call-graph. 

From our investigation of basic-only's CHA, it is probably safe to disregard interprocedural basic-only mock invocation results.


\paragraph{More Sophisticated Analyses}
Analyzing the Doop interprocedural results among the three more advanced base analyses (CI, CIPP and 1-object-sens), we see that they report the same number of interprocedural mock invocations in 5 out of 9 benchmarks. CI and CIPP report the same number of interprocedural mock invocations in 2 more benchmarks that 1-object-sensitive base analysis timed out in generating the call graph.

% CI and CIPP report more mock invocations in 7 out of the 9 benchmarks compared to basic-only. For benchmarks \textsc{flink-core}, \textsc{jsonschema2pojo-core}, \textsc{mybatis} and \textsc{vraptor-core}, CI and CIPP report significantly more mock invocations than basic-only. Our investigation in Section~\ref{sec:basic-only-vs-others} suggests that such discrepancies are due to the fact that basic-only reports edges to abstract methods and not edges to concrete implementations of these abstract methods. We need to have more discussions with doop authors before giving a definite conclusion, but we believe that the basic-only analysis needs improvement before it can be used to produce a reliable call graph.

We can observe that CI and CIPP have comparable run-times and mock invocation counts, with CI having slightly higher run-times, both intraprocedurally and interprocedurally. This makes sense, as CIPP is ``context-insensitive with an enhancement for low-hanging fruit: methods that have their params flow to their return value get a methods that have their params flow to their return value get a 1-obj treatment"~\cite{yanniss}. Data presented in Table~\ref{tab:doop-callgraph-all-counts} demonstrates that the call graph sizes are comparable for CI and CIPP, with CI's call graphs generally slightly bigger as expected. 

On the other hand, since 1-object-sensitive base analysis is a more sophisticated analysis, it spends more time on building the call graph (3 benchmarks ending up with timed-out runs on building the call graph). We feed the same input to all four base analyses, but a more sophisticated analysis, like 1-object-sensitive, returns a much smaller call-graph. We believe it produces a better defined call-graph and thus executes faster on the actual mock analysis. However, we need to communicate to Doop developers more and have a better understanding of the logic involved in these sophisticated analyses before making a conclusion.

%We have successfully executed our Doop analysis with different base analyses. We chose to report numbers for the context-insensitive base analysis here as it matches our own analysis. (It would also be possible, but require significantly more effort, to adapt our analysis to carry around context.)

%--- , indicates the removal of mock invocations from call graph would improve the call graph's accuracy on method coverage for the benchmarks on the high end of the mock invocation percentage. 

% We are currently investigating the performance of intra-procedural basic-only analysis, trying to understand why it would spend more time strictly on mock analysis than CI and CIPP.

%the number of mock invocations correlates with the runtime; taking a bit more effort to compute a better call graph may well pay off in terms of overall analysis time. We suspect that the interprocedural analysis is especially slow for mybatis because we also analyze its 50 dependencies; that count is at the high end among our benchmarks.

\begin{table*}
	\centering
	\caption[Benchmarks' LOC plus Soot and Doop analysis run-times.]{Our suite of 8 open-source benchmarks (8000--117000 LOC) plus our microbenchmark. Soot and Doop analysis run-times.}
	%	\begin{adjustbox}{width=0.1\textwidth}
	\vspace*{.5em}
	\resizebox{\columnwidth}{!}{%
		\begin{tabular}{lrrrrrr}
			\toprule
			Benchmark & Total LOC & Test LOC & \thead{Soot intraproc \\ total time (s)} & \thead{Doop intraproc \\ total time (s)} & \thead{Soot intraproc \\ mock analysis (s)}  & \thead{Doop intraproc \\ mock analysis (s)} \\
			\midrule
			bootique-2.0.B1-bootique           		&  15530   & 8595   & 58  & 2810  &  0.276   & 19.93       \\
			commons-collections4-4.4           		&  65273   & 36318  & 114 & 694   &  0.386   & 14.20       \\
			flink-core-1.13.0-rc1           		&  117310  & 49730  & 341 & 1847  &  0.415   & 27.21        \\
			jsonschema2pojo-core-1.1.1         		&  8233    & 2885   & 313 & 1005   &  0.282   & 29.33       \\
			maven-core-3.8.1   		           		&  38866   & 11104  & 183 & 588   &  0.276   & 19.49        \\
			micro-benchmark         		  		&  954     & 883	& 47  & 387   &  0.130   & 11.73        \\
			mybatis-3.5.6         		  			&  68268   & 46334  & 500 & 4477  &  0.662   & 59.83        \\
			quartz-core-2.3.1        	  			&  35355   & 8423   & 155 & 736   &  0.231   & 21.06     \\
			vraptor-core-3.5.5         	  			&  34244   & 20133  & 371 & 1469  &  0.455   & 34.95      \\
			\bottomrule
			Total         	  						&  384033  & 184405 & 2082 & 14013 &  3.123  & 237.73     \\
		\end{tabular}
	}
	%	\end{adjustbox}
	\label{tab:runtimes}
\end{table*}

\begin{table*}
	\centering
	\caption[Counts of Field Mock Objects.]{Counts of Field Mock Objects defined via \protect \texttt{@Mock} annotation, in the constructors, and in \texttt{@Before} methods, in each benchmark's test suite.}
	%	\begin{adjustbox}{width=0.1\textwidth}
	\vspace*{.5em}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{lrrrr}
			\toprule
			Benchmark & \thead{\# of Annotated \\ Field Mock Objects} & \thead{\# of Field Mock Objects \\ defined in the \texttt{<init>} constructor}  & \thead{\# of Field Mock Objects \\ defined in @Before methods} \\
			\midrule
			bootique-2.0.B1-bootique           		&  0        &  0    & 8        \\
			commons-collections4-4.4          		&  0        &  0    & 0        \\
			flink-core-1.13.0-rc1           		&  0        &  0    & 0        \\
			jsonschema2pojo-core-1.1.1           	&  26       &  126  & 0        \\
			maven-core-3.8.1	           			&  7        &  0    & 1        \\
			micro-benchmark         		  		&  2        &  0    & 29        \\
			mybatis-3.5.6         		  			&  41       &  0    & 0        \\
			quartz-core-2.3.1         	  			&  0     	&  0    & 0      \\
			vraptor-core-3.5.5         	  			&  263      &  128  & 83       \\
			\bottomrule
		\end{tabular}
	}
	%	\end{adjustbox}
	\label{tab:field-mocks}
\end{table*}

\begin{table*}
	\centering
	\caption[Counts of Mock Objects in Test-Related Methods.]{Counts of Test-Related (Test/Before/After) methods in public concrete test classes, along with counts of mocks, mock-containing arrays, and mock-containing collections, reported by Soot intraprocedural analysis.}
	%	\begin{adjustbox}{width=0.1\textwidth}
	\vspace*{.5em}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{lrrrr}
			\toprule
			Benchmark & \thead{\# of Test-Related \\ Methods} & \thead{\# of Test-Related \\ Methods with \\ mocks (intra)}  & \thead{\# of Test-Related \\ Methods with \\ mock-containing\\ arrays (intra)} & \thead{\# of Test-Related \\ Methods with \\ mock-containing\\ collections (intra)} \\
			\midrule
			bootique-2.0.B1-bootique           		&  420        &  32  & 7 & 0       \\
			commons-collections4-4.4          		&  1152       &  3   & 1 & 1       \\
			flink-core-1.13.0-rc1           		&  1091       &  4   & 0 & 0       \\
			jsonschema2pojo-core-1.1.1           	&  145        &  76  & 1 & 0       \\
			maven-core-3.8.1	           			&  337        &  24  & 0 & 0       \\
			micro-benchmark         		  		&  59         &  43  & 7 & 25       \\
			mybatis-3.5.6         		  			&  1769       &  330 & 3 & 0       \\
			
			quartz-core-2.3.1         	  			&  218     	  &  7   & 0 & 0      \\
			vraptor-core-3.5.5         	  			&  1119       &  565 & 15 & 0      \\
			\bottomrule
			Total        	  						&  6310       &  1084  & 34 & 26    \\
		\end{tabular}
	}
	%	\end{adjustbox}
	\label{tab:mocks}
\end{table*}

\begin{landscape}
	\begin{table*}
		\ra{1.2}
		\centering
		\caption[Number of Mock Invocations in Each Benchmark.]{Number of InstanceInvokeExprs on Mock objects analyzed by Soot and Doop, and Total Number of InstanceInvokeExprs, in each benchmark's test suite. ``--''~=~timed out after 90 minutes. Runs [mybatis, basic-only] and [flink-core, 1-object-sensitive] take close to 90 minutes and sometimes time out.}
		\vspace*{.5em}
		\begin{tabular}{@{}lrrcrrrrcrrrr@{}} \toprule
			Benchmark & \thead{Total \\ Number of \\ Invocations} & \thead{Mock Invokes \\ intraproc (Soot)}
			& \phantom{abc} & \multicolumn{4}{c}{\thead{Mock Invokes \\ intraproc (Doop)}} & \phantom{abc} & \multicolumn{4}{c}{\thead{Mock Invokes \\ interproc (Doop)}}
			\\
			\cmidrule{5-8} \cmidrule{10-13}
			& & & & \thead{basic\\-only} & CI & CIPP & \thead{1-obj\\-sens} & & \thead{basic\\-only} & CI & CIPP & \thead{1-obj\\-sens} \\ \midrule
			\csvreader[head to column names, late after line=\\]
			{Data/DoopAndSootMockCounts.csv}{}%
			{\csvcoli&\csvcolii&\csvcoliii&&\csvcoliv&\csvcolv&\csvcolvi&\csvcolvii& &\csvcolviii&\csvcolix&\csvcolx&\csvcolxi}
			\bottomrule
		\end{tabular}
		\label{tab:invokes}
	\end{table*} 
\end{landscape}

%\begin{table*}
%	\centering
%	\caption{Comparison of Number of InstanceInvokeExprs on Mock objects analyzed by Soot and Doop, and Total Number of InstanceInvokeExprs, in each benchmark's test suite. N/A = timed out after 90 minutes.}
%	%	\begin{adjustbox}{width=0.1\textwidth}
%	\begin{tabular}{lrrrrrr}
%		\toprule
%		Benchmark & \thead{Total Number \\ of Invocations} & \thead{Mock Invokes \\ intraproc (Soot)} & \thead{Basic-only, \\ intraproc (Doop)} & \thead{Context-insensitive, \\ intraproc (Doop)} &  \thead{Basic-only, \\ interproc (Doop)} &\thead{Context-insensitive, \\ interproc (Doop)} \\
%		\midrule
%		bootique-2.0.B1-bootique           		&  3366     &  99   & 99    & 99   & 120   & 122    \\
%		commons-collections4-4.4       			&  12753    &  11   & 0     &  3   & 0    & 23   \\
%		flink-core-1.13.0-rc1           		&  11923    &  40   & 40    & 40   & 1262  & 1389   \\
%		jsonschema2pojo-core-1.1.1      	     	&  1896     &  276  & 282   & 282  & 462   & 604   \\
%		maven-core-3.8.1           			&  4072     &  23   & 23    & 23   & 31    & 39  \\
%		microbenchmark         		  		&  471      &  108  & 123   & 123  & 132   & 132   \\
%		mybatis-3.5.6         		  		&  19232    &  575  & N/A   & 577  &  N/A  & 1345     \\
%		quartz-core-2.3.1       	  		&  3436     &  21   & 21    & 21   & 23    & 31    \\
%		vraptor-core-3.5.51        	  		&  5868     &  942  & 963   & 962  & 1301  & 1630   \\
%		\bottomrule
%		Total        	  				&  63017    & 2095  & N/A   & 2130  & N/A  & 5315   \\
%	\end{tabular}
%	%	\end{adjustbox}
%	\label{tab:invokes}
%\end{table*}

%& & \thead{basic\\-only} & $\sigma$ & CI & $\sigma$ & CIPP & $\sigma$ & \thead{1-obj\\-sens} & $\sigma$ \\

\clearpage

\begin{table*}
	\centering
	\caption[Intraprocedural Doop Analysis-Only Run-time.]{Intraprocedural Doop analysis-only run-time (in seconds) after basic-only, context-insensitive, context-insensitive-plusplus and 1-object-sensitive base analyses. \protect\\ ``--''~=~timed out after 90 minutes. Runs [mybatis, basic-only] and [flink-core, 1-object-sensitive] take close to 90 minutes and sometimes time out.}
	\vspace*{.5em}
	\begin{tabular}{@{}lrrrrrrrr} \toprule
		Benchmark & \multicolumn{8}{c}{intraproc}\\
		\cmidrule{2-9}
		& \thead{basic\\-only} & $\sigma$ & CI & $\sigma$ & CIPP & $\sigma$ & \thead{1-obj\\-sens} & $\sigma$ \\ \midrule 
		
		\csvreader[head to column names, late after line=\\]
		{Data/HyperfineRuntimeWithAvgsINTRA.csv}{}%
		{\csvcoli&\csvcolii&{\scriptsize \csvcoliii}&\csvcoliv&{\scriptsize \csvcolv} &\csvcolvi&{\scriptsize \csvcolvii}&\csvcolviii &{\scriptsize \csvcolix}}
		\bottomrule
	\end{tabular}
	\label{tab:doop-intra-runtimes}
\end{table*}

\begin{table*}
	\centering
	\caption[Interprocedural Doop Analysis-Only Run-time.]{Interprocedural Doop analysis-only run-time (in seconds) after basic-only, context-insensitive, context-insensitive-plusplus and 1-object-sensitive base analyses. \protect\\ ``--''~=~timed out after 90 minutes. Runs [mybatis, basic-only] and [flink-core, 1-object-sensitive] take close to 90 minutes and sometimes time out.}
	\vspace*{.5em}
	\begin{tabular}{@{}lrrrrrrrr} \toprule
		Benchmark & \multicolumn{8}{c}{interproc}\\
		\cmidrule{2-9}
		& \thead{basic\\-only} & $\sigma$ & CI & $\sigma$ & CIPP & $\sigma$ & \thead{1-obj\\-sens} & $\sigma$ \\ \midrule 
		
		\csvreader[head to column names, late after line=\\]
		{Data/HyperfineRuntimeWithAvgsINTER.csv}{}%
		{\csvcoli&\csvcolii&{\scriptsize \csvcoliii}&\csvcoliv&{\scriptsize \csvcolv} &\csvcolvi&{\scriptsize \csvcolvii}&\csvcolviii &{\scriptsize \csvcolix}}
		\bottomrule
	\end{tabular}
	\label{tab:doop-inter-runtimes}
\end{table*}


\begin{table*}
	\centering
	\caption[Call Graph Statistics.]{Call graph statistics: total number of source classes and target classes from interprocedural Doop analysis with basic-only, context-insensitive, context-insensitive-plusplus, and 1-object-sensitive base analyses. ``--''~=~timed out after 90 minutes. Runs [mybatis, basic-only] and [flink-core, 1-object-sensitive] take close to 90 minutes and sometimes time out.}
	\vspace*{.5em}
	\begin{tabular}{@{}lrrrrcrrrr} \toprule
		Benchmark & \multicolumn{4}{c}{Source Classes} & \phantom{abc} & \multicolumn{4}{c}{Target Classes}
		\\
		\cmidrule{2-5} \cmidrule{7-10}
		& \thead{basic\\-only} & CI & CIPP & \thead{1-obj\\-sens} & & \thead{basic\\-only} & CI & CIPP & \thead{1-obj\\-sens} \\ \midrule
		\csvreader[head to column names, late after line=\\]
		{Data/CallGraphEdgeClassCounts.csv}{}%
		{\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&&\csvcolvi&\csvcolvii&\csvcolviii&\csvcolix}
		\bottomrule
	\end{tabular}
	\label{tab:doop-callgraph-all-counts}
\end{table*}

\begin{table*}
	\centering
	\caption[Call Graph Statistics Excluding Dependencies.]{Call graph statistics: total number of source classes that are application classes (i.e., excluding classes from dependencies or libraries) and total number of target classes reached from application classes by interprocedural Doop analysis with basic-only, context-insensitive, context-insensitive-plusplus, and 1-object-sensitive base analyses. ``--''~=~timed out after 90 minutes. Runs [mybatis, basic-only] and [flink-core, 1-object-sensitive] take close to 90 minutes and sometimes time out.}
	\vspace*{.5em}
	\begin{tabular}{@{}lrrrrcrrrr} \toprule
		Benchmark & \multicolumn{4}{c}{Source Classes} & \phantom{abc} & \multicolumn{4}{c}{Target Classes}
		\\
		\cmidrule{2-5} \cmidrule{7-10}
		& \thead{basic\\-only} & CI & CIPP & \thead{1-obj\\-sens} & & \thead{basic\\-only} & CI & CIPP & \thead{1-obj\\-sens} \\ \midrule
		\csvreader[head to column names, late after line=\\]
		{Data/CallGraphEdgePackageClassCounts.csv}{}%
		{\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&&\csvcolvi&\csvcolvii&\csvcolviii&\csvcolix}
		\bottomrule
	\end{tabular}
	\label{tab:doop-callgraph-package-counts}
\end{table*}

%\begin{table*}
%	\centering
%	\caption{Doop analysis-only runtime after basic-only, context-insensitive, context-insensitive-plusplus and 1-object-sensitive base analyses. N/A = timed out after 90 minutes.}
%	%	\begin{adjustbox}{width=0.1\textwidth}
%	\begin{tabular}{lrrrrrr}
%		\toprule
%		Benchmark & \thead{Basic-only, \\ intraproc (s)} & \thead{Context-insensitive, \\ intraproc (s)} & \thead{Basic-only, \\ interproc (s)}  & \thead{Context-insensitive, \\ interproc (s)}  \\
%		\midrule
%		bootique-2.0.B1-bootique           		& 15.71  & 16.81 &  24.26    &  20.20     \\
%		commons-collections4-4.4           		& 17.42  & 12.26 &  21.79    &  15.36        \\
%		flink-core-1.13.0-rc1           		& 24.67  & 25.30 &  71.67    &  66.10         \\
%		jsonschema2pojo-core-1.1.1         		& 25.98  & 26.27 &  42.14    &  39.21         \\
%		maven-core-3.8.1   		        	& 18.01  & 16.34 &  25.49    &  22.09          \\
%		micro-benchmark         			& 10.97  & 10.50 &  12.51    &  12.53        \\
%		mybatis-3.5.6         		  		&  N/A   & 51.25 &   N/A     & 183.86          \\
%		quartz-core-2.3.1        	  		& 17.72  & 19.83 &  22.99    &  21.14        \\
%		vraptor-core-3.5.5         	  		& 22.10  & 23.81 &  66.73    & 146.09       \\
%		\bottomrule
%	\end{tabular}
%	%	\end{adjustbox}
%	\label{tab:doop-runtimes}
%\end{table*}
