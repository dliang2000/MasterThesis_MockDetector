%======================================================================
\chapter{Related Work}
\label{chap:related}
%====================================================================== 

We discuss related work in the areas of declarative versus imperative static analysis, treatment of containers, and taint analysis.

\section{Imperative vs Declarative}

Kildall contributed perhaps the first dataflow analysis~\cite{kildall73:_unified_approac_global_progr_optim} as the concept is understood today, describing an algorithm for intraprocedural constant propagation and common subexpression elimination. His algorithm, operating on the program graph, is described in quite imperative pseudocode (and proven to terminate). In some sense, implementing algorithms imperatively is the default, and doesn't need further discussion, except to point out that program analysis frameworks such as Soot~\cite{Vallee-Rai:1999:SJB:781995.782008} provide libraries that can ease the implementation burden.

To our knowledge, Corsini et al did some of the first work in declarative program analysis~\cite{corsini93:_effic}; however, that work performed abstract interpretation on (tiny) logic programs rather than imperative programs. Dawson et al~\cite{dawson96:_pract_progr_analy_using_gener} did similar work. Around the same time, Reps proposed~\cite{Reps1995} a declarative analysis to perform on-demand versions of interprocedural program analyses, which is similar to what we have here; however, we compute all of the analysis results rather than performing an on-demand analysis. CodeQuest by Hajijev et al~\cite{hajiyev06} also allows developers to perform AST-level code queries using a declarative query language. {\sc Dimple$^+$}~\cite{benton07:_inter_scalab_declar_progr_analy}\cite[Chapter 3]{benton08:_fast_effec_progr_analy_objec_level_paral} by Benton and Fischer may be closest to what we are advocating as the declarative analysis approach. While Benton's dissertation presents a simple {\sc Dimple$^+$} implementation of Andersen's points-to analysis, the {\sc Dimple$^+$} work does not have Doop's sophisticated pointer analysis available to it. Soufflé, by Scholz et al~\cite{scholz16:_fast_large_scale_progr_analy_datal}, advocates for declarative static analysis (but without comparing it directly to an imperative approach as we do here), and presents performance optimizations needed to achieve this goal.
Finally, Doop~\cite{bravenboer09:_stric_declar_specif_sophis_point_analy}, which is now primarily implemented with a Soufflé backend, is perhaps the most powerful extant declarative program analysis, and focuses on expressing sophisticated pointer analyses in Datalog. 



% implementation note: Reps's approach is much more complicated than what we have in Doop. Perhaps Doop's use of SSA and simulation of phi nodes allows it to use much simpler rules, or maybe it's the specific analyses that are being implemented. e.g. for Doop, which computes an overapproximation, merging the two branches using the virtual phi node (simulated as "x = phi(x1,x2) => x = x1; x = x2") works just fine.

In terms of comparing implementations, Prakash et al~\cite{prakash21:_effec_progr_repres_point_analy} compare pointer analyses as provided by Doop and Wala; in some sense, the present work is similar to that work in that both works compare two frameworks. However, that work compares empirical results from two families of pointer analysis implementations (and finds that the specific intermediate representation used doesn't change the results much), while we discuss the process of implementing a static analysis declaratively versus imperatively. Like us, they note that Doop is difficult to incorporate into a program transformation framework (it works better in standalone mode) while Wala's results are readily available; a similar result applies to any result that a Soot-based data flow analysis produces as compared to a Doop-based declarative analysis.

\section{Treatment of Containers}

In this work, we use coarse-grained abstractions for containers, consistent with the approach from Chu et al~\cite{chu12:_collec_disjoin_analy}. In our experience, test cases do not perform sophisticated container manipulations where it would be necessary to track exactly which elements of a container are mocks. Were such an analysis necessary, we could use the fine-grained container client analysis by Dillig et at~\cite{dillig11:_precis_reason_progr_using_contain}.

\section{Taint Analysis}

Like many other static analyses, our mock analysis can be seen as a variant of a static taint analysis: sources are mock creation methods, while sinks are method invocations. There are no sanitizers in our case. However, for a taint analysis, there is usually a small set of sink methods, while in our case, every method invocation in a test method is a potential sink. Additionally, the goal of our analysis (detecting possible mocks) is different in that it is not security-sensitive, so the balance between false positives and false negatives is different---it is less critical to not miss any potential mock invocations, whereas missing a whole class of tainted methods would often be unacceptable.
